{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umtjXBD0VOGB",
        "outputId": "4ea17e7b-1ed7-43ef-b983-06c9ca4f80ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ADNet-VIN' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/baovin/ADNet-VIN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SimpleITK monai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfdFrAtvVf8L",
        "outputId": "2c7334fa-824c-4342-a9af-7d7308dd7716"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_root', default= \"/content/ADNet-VIN/dataloading/\", type=str)\n",
        "    parser.add_argument('--save_root', default = \"/content/ADNet-VIN/log_out\", type=str)\n",
        "    parser.add_argument('--dataset', default='CHAOST2', type=str)\n",
        "    parser.add_argument('--n_sv', default=1, type=int)\n",
        "    parser.add_argument('--fold', default=1, type=int)\n",
        "\n",
        "    # Training specs.\n",
        "    parser.add_argument('--max_slices', default=10, type=int)\n",
        "    parser.add_argument('--workers', default=4, type=int)\n",
        "    parser.add_argument('--steps', default=15000, type=int)\n",
        "    parser.add_argument('--n_shot', default=1, type=int)\n",
        "    parser.add_argument('--n_query', default=1, type=int)\n",
        "    parser.add_argument('--n_way', default=1, type=int)\n",
        "    parser.add_argument('--batch-size', default=1, type=int)\n",
        "    parser.add_argument('--max_iterations', default=50, type=int)\n",
        "    parser.add_argument('--lr', default=1e-3, type=float)\n",
        "    parser.add_argument('--lr_gamma', default=0.95, type=float)\n",
        "    parser.add_argument('--momentum', default=0.9, type=float)\n",
        "    parser.add_argument('--weight-decay', default=0.0005, type=float)\n",
        "    parser.add_argument('--seed', default=None, type=int)\n",
        "    parser.add_argument('--bg_wt', default=0.1, type=float)\n",
        "    parser.add_argument('--t_loss_scaler', default=1.0, type=float)\n",
        "    parser.add_argument('--min_size', default=200, type=int)\n",
        "\n",
        "    # parser.add_argument('--max_iterations', default=50, type=int)\n",
        "\n",
        "    # Inference specs.\n",
        "    parser.add_argument('--all_slices', default=True, type=bool)\n",
        "    parser.add_argument('--EP1', default=True, type=bool)\n",
        "\n",
        "    return parser.parse_known_args()  # Use parse_known_args to avoid unrecognized args issue\n",
        "\n",
        "args, unknown = parse_arguments()  # 'unknown' will hold the unrecognized args like '-f'\n",
        "\n",
        "# Deterministic setting for reproducibility.\n",
        "if args.seed is not None:\n",
        "    random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "print(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83UYVXmVVvau",
        "outputId": "7713a81c-2cd9-4429-d5ec-9b07d829e5f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_root='/content/ADNet-VIN/dataloading/', save_root='/content/ADNet-VIN/log_out', dataset='CHAOST2', n_sv=1, fold=1, max_slices=10, workers=4, steps=15000, n_shot=1, n_query=1, n_way=1, batch_size=1, max_iterations=50, lr=0.001, lr_gamma=0.95, momentum=0.9, weight_decay=0.0005, seed=None, bg_wt=0.1, t_loss_scaler=1.0, min_size=200, all_slices=True, EP1=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/ADNet-VIN/dataloading/')"
      ],
      "metadata": {
        "id": "WeUQ5HO_WlXw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset_specifics import get_folds, sample_xy, get_label_names"
      ],
      "metadata": {
        "id": "jM0XGNdQWrpe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgjx0SBvmfxB",
        "outputId": "7e982f1d-6b3c-4b24-ecd0-71473e4a3f73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip superdix.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ho3UyGqmiMe",
        "outputId": "b382d291-d947-4875-c162-eb6aaeffd518"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  superdix.zip\n",
            "   creating: superdix/\n",
            "  inflating: superdix/superpix-MIDDLE_1.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_10.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_13.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_15.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_19.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_2.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_20.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_21.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_22.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_3.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_31.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_32.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_33.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_34.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_36.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_37.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_38.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_39.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_5.nii.gz  \n",
            "  inflating: superdix/superpix-MIDDLE_8.nii.gz  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIGKXtMMXHcV",
        "outputId": "fd186c0a-78b6-4e04-f8b0-a3691ff293c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/ADNet-VIN/dataloading/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzsft3HTXK4L",
        "outputId": "188d9990-384d-4ad4-dc88-5054d8ac19b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ADNet-VIN/dataloading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "import os\n",
        "import SimpleITK as sitk\n",
        "import random\n",
        "import numpy as np\n",
        "# from .dataset_specifics import *\n",
        "from monai.transforms.spatial.dictionary import Rand3DElasticd\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        # reading the paths\n",
        "        if args.dataset == 'CMR':\n",
        "            self.image_dirs = glob.glob(os.path.join(args.data_root, 'cmr_MR_normalized/image*'))\n",
        "        elif args.dataset == 'CHAOST2':\n",
        "            self.image_dirs = glob.glob(os.path.join(args.data_root, 'chaos_MR_T2_normalized/image*'))\n",
        "        self.image_dirs = sorted(self.image_dirs, key=lambda x: int(x.split('_')[-1].split('.nii.gz')[0]))\n",
        "\n",
        "        # remove test fold!\n",
        "        self.FOLD = get_folds(args.dataset)\n",
        "        self.image_dirs = [elem for idx, elem in enumerate(self.image_dirs) if idx in self.FOLD[args.fold]]\n",
        "\n",
        "        # split into support/query\n",
        "        self.support_dir = self.image_dirs[-1]\n",
        "        self.image_dirs = self.image_dirs[:-1]  # remove support\n",
        "        self.label = None\n",
        "\n",
        "        # evaluation protocol\n",
        "        self.EP1 = args.EP1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_dirs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = self.image_dirs[idx]\n",
        "        img = sitk.GetArrayFromImage(sitk.ReadImage(img_path))\n",
        "        img = (img - img.mean()) / img.std()\n",
        "        img = np.stack(1 * [img], axis=0)\n",
        "\n",
        "        lbl = sitk.GetArrayFromImage(\n",
        "            sitk.ReadImage(img_path.split('image_')[0] + 'label_' + img_path.split('image_')[-1]))\n",
        "        lbl[lbl == 200] = 1\n",
        "        lbl[lbl == 500] = 2\n",
        "        lbl[lbl == 600] = 3\n",
        "        lbl = 1 * (lbl == self.label)\n",
        "\n",
        "        sample = {'id': img_path}\n",
        "\n",
        "        # Evaluation protocol 1.\n",
        "        if self.EP1:\n",
        "            idx = lbl.sum(axis=(1, 2)) > 0\n",
        "            sample['image'] = torch.from_numpy(img[idx])\n",
        "            sample['label'] = torch.from_numpy(lbl[idx])\n",
        "\n",
        "        # Evaluation protocol 2 (default).\n",
        "        else:\n",
        "            sample['image'] = torch.from_numpy(img)\n",
        "            sample['label'] = torch.from_numpy(lbl)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def get_support_index(self, n_shot, C):\n",
        "        \"\"\"\n",
        "        Selecting intervals according to Ouyang et al.\n",
        "        \"\"\"\n",
        "        if n_shot == 1:\n",
        "            pcts = [0.5]\n",
        "        else:\n",
        "            half_part = 1 / (n_shot * 2)\n",
        "            part_interval = (1.0 - 1.0 / n_shot) / (n_shot - 1)\n",
        "            pcts = [half_part + part_interval * ii for ii in range(n_shot)]\n",
        "\n",
        "        return (np.array(pcts) * C).astype('int')\n",
        "\n",
        "    def getSupport(self, label=None, all_slices=True, N=None):\n",
        "        if label is None:\n",
        "            raise ValueError('Need to specify label class!')\n",
        "\n",
        "        img_path = self.support_dir\n",
        "        img = sitk.GetArrayFromImage(sitk.ReadImage(img_path))\n",
        "        img = (img - img.mean()) / img.std()\n",
        "        img = np.stack(1 * [img], axis=0)\n",
        "\n",
        "        lbl = sitk.GetArrayFromImage(\n",
        "            sitk.ReadImage(img_path.split('image_')[0] + 'label_' + img_path.split('image_')[-1]))\n",
        "        lbl[lbl == 200] = 1\n",
        "        lbl[lbl == 500] = 2\n",
        "        lbl[lbl == 600] = 3\n",
        "        lbl = 1 * (lbl == label)\n",
        "\n",
        "        sample = {}\n",
        "        if all_slices:\n",
        "\n",
        "            sample['image'] = torch.from_numpy(img)[None]\n",
        "            sample['label'] = torch.from_numpy(lbl)[None]\n",
        "\n",
        "            # target = np.where(lbl.sum(axis=(-2, -1)) > 0)[0]\n",
        "            # mask = np.zeros(lbl.shape) == 1\n",
        "            # mask[target.astype('float').mean().astype('int')] = True\n",
        "            # sample['label'] = torch.from_numpy((mask*1)*lbl)[None]\n",
        "\n",
        "        else:\n",
        "            # select N labeled slices\n",
        "            if N is None:\n",
        "                raise ValueError('Need to specify number of labeled slices!')\n",
        "            idx = lbl.sum(axis=(1, 2)) > 0\n",
        "            idx_ = self.get_support_index(N, idx.sum())\n",
        "\n",
        "            sample['image'] = torch.from_numpy(img[:, idx][:, idx_])[None]\n",
        "            sample['label'] = torch.from_numpy(lbl[idx][idx_])[None]\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.n_shot = args.n_shot\n",
        "        self.n_way = args.n_way\n",
        "        self.n_query = args.n_query\n",
        "        self.n_sv = args.n_sv\n",
        "        self.max_iter = args.max_iterations\n",
        "        self.min_size = args.min_size\n",
        "        self.max_slices = args.max_slices\n",
        "\n",
        "        # reading the paths (leaving the reading of images into memory to __getitem__)\n",
        "        if args.dataset == 'CMR':\n",
        "            self.image_dirs = glob.glob(os.path.join(args.data_root, 'cmr_MR_normalized/image*'))\n",
        "        elif args.dataset == 'CHAOST2':\n",
        "            self.image_dirs = glob.glob(os.path.join(args.data_root, 'chaos_MR_T2_normalized/image*'))\n",
        "        self.image_dirs = sorted(self.image_dirs, key=lambda x: int(x.split('_')[-1].split('.nii.gz')[0]))\n",
        "        self.sprvxl_dirs = glob.glob(os.path.join(args.data_root, 'superdix/', 'super*'))\n",
        "        # self.sprvxl_dirs = glob.glob(os.path.join(args.data_root, 'supervoxels_' + str(args.n_sv), 'super*'))\n",
        "        self.sprvxl_dirs = sorted(self.sprvxl_dirs, key=lambda x: int(x.split('_')[-1].split('.nii.gz')[0]))\n",
        "\n",
        "        # remove test fold!\n",
        "        self.FOLD = get_folds(args.dataset)\n",
        "        self.image_dirs = [elem for idx, elem in enumerate(self.image_dirs) if idx not in self.FOLD[args.fold]]\n",
        "        self.sprvxl_dirs = [elem for idx, elem in enumerate(self.sprvxl_dirs) if idx not in self.FOLD[args.fold]]\n",
        "        self.N = len(self.image_dirs)\n",
        "\n",
        "        # read images\n",
        "        self.images = {}\n",
        "        self.sprvxls = {}\n",
        "        self.valid_spr_slices = {}\n",
        "        for image_dir, sprvxl_dir in zip(self.image_dirs, self.sprvxl_dirs):\n",
        "            img = sitk.ReadImage(image_dir)\n",
        "            self.res = img.GetSpacing()\n",
        "            img = sitk.GetArrayFromImage(img)\n",
        "            self.images[image_dir] = torch.from_numpy(img)\n",
        "            spr = torch.from_numpy(sitk.GetArrayFromImage(sitk.ReadImage(sprvxl_dir)))\n",
        "            self.sprvxls[sprvxl_dir] = spr\n",
        "\n",
        "            unique = list(torch.unique(spr))\n",
        "            unique.remove(0)\n",
        "            self.valid_spr_slices[image_dir] = []\n",
        "            for val in unique:\n",
        "                spr_val = (spr == val)\n",
        "\n",
        "                n_slices = min(spr_val.shape[0], self.max_slices)\n",
        "                sample_list = []\n",
        "                for r in range(spr_val.shape[0] - (n_slices - 1)):\n",
        "                    sample_idx = torch.arange(r, r + n_slices).tolist()\n",
        "                    candidate = spr_val[sample_idx]\n",
        "                    if candidate.sum() > self.min_size:\n",
        "                        sample_list.append(sample_idx)\n",
        "                if len(sample_list) > 0:\n",
        "                    self.valid_spr_slices[image_dir].append((val, sample_list))\n",
        "\n",
        "        # set transformation details\n",
        "        rad = 5 * (np.pi / 180)\n",
        "        self.rand_3d_elastic = Rand3DElasticd(\n",
        "            keys=(\"img\", \"seg\"),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "            sigma_range=(5, 5),\n",
        "            magnitude_range=(0, 0),\n",
        "            prob=1.0,  # because probability controlled by this class\n",
        "            rotate_range=(rad, rad, rad),\n",
        "            shear_range=(rad, rad, rad),\n",
        "            translate_range=(5, 5, 1),\n",
        "            scale_range=((-0.1, 0.2), (-0.1, 0.2), (-0.1, 0.2)),\n",
        "            # as_tensor_output=True,\n",
        "            device='cpu')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_iter\n",
        "\n",
        "    def gamma_tansform(self, img):\n",
        "        gamma_range = (0.5, 1.5)\n",
        "        gamma = torch.rand(1) * (gamma_range[1] - gamma_range[0]) + gamma_range[0]\n",
        "        cmin = img.min()\n",
        "        irange = (img.max() - cmin + 1e-5)\n",
        "\n",
        "        img = img - cmin + 1e-5\n",
        "        img = irange * torch.pow(img * 1.0 / irange, gamma)\n",
        "        img = img + cmin\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # sample patient idx\n",
        "        pat_idx = random.choice(range(len(self.image_dirs)))\n",
        "\n",
        "        # get image/supervoxel volume from dictionary\n",
        "        img = self.images[self.image_dirs[pat_idx]]\n",
        "        sprvxl = self.sprvxls[self.sprvxl_dirs[pat_idx]]\n",
        "\n",
        "        # normalize\n",
        "        img = (img - img.mean()) / img.std()\n",
        "\n",
        "        # sample supervoxel\n",
        "        valid = self.valid_spr_slices[self.image_dirs[pat_idx]]\n",
        "        cls_idx, candidates = valid[random.randint(0, len(valid) - 1)]\n",
        "\n",
        "        sprvxl = 1 * (sprvxl == cls_idx)\n",
        "\n",
        "        sup_lbl = torch.clone(sprvxl)\n",
        "        qry_lbl = torch.clone(sprvxl)\n",
        "\n",
        "        sup_img = torch.clone(img)\n",
        "        qry_img = torch.clone(img)\n",
        "\n",
        "        # gamma transform\n",
        "        if np.random.random(1) > 0.5:\n",
        "            qry_img = self.gamma_tansform(qry_img)\n",
        "        else:\n",
        "            sup_img = self.gamma_tansform(sup_img)\n",
        "\n",
        "        # geom transform\n",
        "        if np.random.random(1) > 0.5:\n",
        "            res = self.rand_3d_elastic({\"img\": qry_img.permute(1, 2, 0),\n",
        "                                        \"seg\": qry_lbl.permute(1, 2, 0)})\n",
        "\n",
        "            qry_img = res[\"img\"].permute(2, 0, 1)\n",
        "            qry_lbl = res[\"seg\"].permute(2, 0, 1)\n",
        "\n",
        "            # support not tformed\n",
        "            constant_s = random.randint(0, len(candidates) - 1)\n",
        "            idx_s = candidates[constant_s]\n",
        "\n",
        "            k = 50\n",
        "            constant_q = constant_s + random.randint(-min(constant_s, k), min(len(candidates) - constant_s - 1, k))\n",
        "            idx_q = candidates[constant_q]\n",
        "\n",
        "        else:\n",
        "            res = self.rand_3d_elastic({\"img\": sup_img.permute(1, 2, 0),\n",
        "                                        \"seg\": sup_lbl.permute(1, 2, 0)})\n",
        "\n",
        "            sup_img_ = res[\"img\"].permute(2, 0, 1)\n",
        "            sup_lbl_ = res[\"seg\"].permute(2, 0, 1)\n",
        "\n",
        "            constant_q = random.randint(0, len(candidates) - 1)\n",
        "            idx_q = candidates[constant_q]\n",
        "\n",
        "            k = 50\n",
        "            constant_s = constant_q + random.randint(-min(constant_q, k), min(len(candidates) - constant_q - 1, k))\n",
        "            idx_s = candidates[constant_s]\n",
        "            if sup_lbl_[idx_s].sum() > 0:\n",
        "                sup_img = sup_img_\n",
        "                sup_lbl = sup_lbl_\n",
        "\n",
        "        sup_lbl = sup_lbl[idx_s]\n",
        "        qry_lbl = qry_lbl[idx_q]\n",
        "\n",
        "        sup_img = sup_img[idx_s]\n",
        "        qry_img = qry_img[idx_q]\n",
        "\n",
        "        b = 215\n",
        "        k = 0\n",
        "        horizontal_s, vertical_s = sample_xy(sup_lbl, k=k, b=b)\n",
        "        horizontal_q, vertical_q = sample_xy(qry_lbl, k=k, b=b)\n",
        "\n",
        "        sup_img = sup_img[:, horizontal_s:horizontal_s + b, vertical_s:vertical_s + b]\n",
        "        sup_lbl = sup_lbl[:, horizontal_s:horizontal_s + b, vertical_s:vertical_s + b]\n",
        "        qry_img = qry_img[:, horizontal_q:horizontal_q + b, vertical_q:vertical_q + b]\n",
        "        qry_lbl = qry_lbl[:, horizontal_q:horizontal_q + b, vertical_q:vertical_q + b]\n",
        "\n",
        "        sample = {'support_images': torch.stack(1 * [sup_img], dim=0),\n",
        "                  'support_fg_labels': sup_lbl[None],\n",
        "                  'query_images': torch.stack(1 * [qry_img], dim=0),\n",
        "                  'query_labels': qry_lbl}\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "NhTkvx70WHes"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TrainDataset(args)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                                               num_workers=args.workers, pin_memory=True, drop_last=True)\n",
        "print(len(train_dataset))\n",
        "data = train_dataset[0]\n",
        "print(data['support_images'].shape)\n",
        "print(data['support_fg_labels'].shape)\n",
        "print(data['query_images'].shape)\n",
        "print(data['query_labels'].shape)\n",
        "print(len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e36JMuThHkT",
        "outputId": "a04a0543-dc6e-4f1a-bf02-2f7c04ecdcef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "torch.Size([1, 10, 215, 215])\n",
            "torch.Size([1, 10, 215, 215])\n",
            "torch.Size([1, 10, 215, 215])\n",
            "torch.Size([10, 215, 215])\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/ADNet-VIN/')"
      ],
      "metadata": {
        "id": "0xhvgFl_X-Lc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oEbEJa1ZYGKm",
        "outputId": "ba4ddc63-1a9b-4c37-8b50-56b522051c26"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.15.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58IBt8AMb1Bm",
        "outputId": "236afa8c-dc7b-472a-ddb7-885415187837",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8b-nHHgcSxs",
        "outputId": "587d9029-b248-4807-caf5-117151337dda"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1p80RJsghFIKBSLKgtRG94LE38OGY5h4y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfQxOCZxb3nM",
        "outputId": "7fed9609-1f07-4b42-a18e-a6efc46fa6c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p80RJsghFIKBSLKgtRG94LE38OGY5h4y\n",
            "From (redirected): https://drive.google.com/uc?id=1p80RJsghFIKBSLKgtRG94LE38OGY5h4y&confirm=t&uuid=169f12b6-fd48-4397-806d-99b76a02781c\n",
            "To: /content/r3d101_KM_200ep.pth\n",
            "100% 700M/700M [00:12<00:00, 54.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv r3d101_KM_200ep.pth resnext-101-kinetics.pth"
      ],
      "metadata": {
        "id": "yKcrT9iMcpA_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "from functools import partial\n",
        "\n",
        "__all__ = ['ResNeXt', 'resnet50', 'resnet101']\n",
        "\n",
        "\n",
        "def conv3x3x3(in_planes, out_planes, stride=1):\n",
        "    # 3x3x3 convolution with padding\n",
        "    return nn.Conv3d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        bias=False)\n",
        "\n",
        "\n",
        "def downsample_basic_block(x, planes, stride):\n",
        "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "    zero_pads = torch.Tensor(\n",
        "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
        "        out.size(4)).zero_()\n",
        "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "        zero_pads = zero_pads.cuda()\n",
        "\n",
        "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNeXtBottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, cardinality, stride=1,\n",
        "                 downsample=None, dilation=1):\n",
        "        super(ResNeXtBottleneck, self).__init__()\n",
        "        mid_planes = cardinality * int(planes / 32)\n",
        "        self.conv1 = nn.Conv3d(inplanes, mid_planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(mid_planes)\n",
        "        self.conv2 = nn.Conv3d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=dilation,\n",
        "                               groups=cardinality, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(mid_planes)\n",
        "        self.conv3 = nn.Conv3d(\n",
        "            mid_planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 block,\n",
        "                 layers,\n",
        "                 shortcut_type='B',\n",
        "                 cardinality=32,\n",
        "                 replace_stride_with_dilation=None):\n",
        "        self.inplanes = 64\n",
        "        self.dilation = torch.tensor([1, 1, 1])\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)  # NOTE: this is being over-written in the main script!\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n",
        "        self.layer1 = self._make_layer(block, 128, layers[0], shortcut_type,\n",
        "                                       cardinality)\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 256, layers[1], shortcut_type, cardinality, stride=(1, 2, 2), dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 512, layers[2], shortcut_type, cardinality, stride=(1, 1, 1), dilate=replace_stride_with_dilation[1]) # (1, 2, 2) or (2 ,2, 2)\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 1024, layers[3], shortcut_type, cardinality, stride=(1, 1, 1), dilate=replace_stride_with_dilation[2])# (1, 2, 2) or (2 ,2, 2)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self,\n",
        "                    block,\n",
        "                    planes,\n",
        "                    blocks,\n",
        "                    shortcut_type,\n",
        "                    cardinality,\n",
        "                    stride=1,\n",
        "                    dilate=False):\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= torch.tensor(stride)\n",
        "            stride = 1\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(\n",
        "                    downsample_basic_block,\n",
        "                    planes=planes * block.expansion,\n",
        "                    stride=stride)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv3d(\n",
        "                        self.inplanes,\n",
        "                        planes * block.expansion,\n",
        "                        kernel_size=1,\n",
        "                        stride=stride,\n",
        "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.inplanes, planes, cardinality, stride, downsample, dilation=previous_dilation))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, cardinality, dilation=self.dilation))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_fine_tuning_parameters(model, ft_portion):\n",
        "    if ft_portion == \"complete\":\n",
        "        return model.parameters()\n",
        "\n",
        "    elif ft_portion == \"last_layer\":\n",
        "        ft_module_names = []\n",
        "        ft_module_names.append('fc')\n",
        "\n",
        "        parameters = []\n",
        "        for k, v in model.named_parameters():\n",
        "            for ft_module in ft_module_names:\n",
        "                if ft_module in k:\n",
        "                    parameters.append({'params': v})\n",
        "                    break\n",
        "            else:\n",
        "                parameters.append({'params': v, 'lr': 0.0})\n",
        "        return parameters\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported ft_portion: 'complete' or 'last_layer' expected\")\n",
        "\n",
        "\n",
        "def resnext50(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    \"\"\"\n",
        "    model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext101(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    model = nn.DataParallel(model, device_ids=[0, ])\n",
        "    load = True\n",
        "    if load:\n",
        "        print('Loading pre-trained weights!')\n",
        "        # pretrained_dict = torch.load('./pretrained_model/kinetics_resnext_101_RGB_16_best.pth', map_location='cpu')\n",
        "        # pretrained_dict = torch.load('./pretrained_model/jester_resnext_101_RGB_16_best.pth', map_location='cpu')\n",
        "        pretrained_dict = torch.load('/content/resnext-101-kinetics.pth', map_location='cpu')\n",
        "\n",
        "        model_dict = model.state_dict()\n",
        "\n",
        "        # 1. filter out unnecessary keys\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict[\"state_dict\"].items() if k in model_dict}\n",
        "\n",
        "        # 2. overwrite entries in the existing state dict\n",
        "        model_dict.update(pretrained_dict)\n",
        "        # 3. load the new state dict\n",
        "        model.load_state_dict(model_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext152(**kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    \"\"\"\n",
        "    model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# import SimpleITK as sitk\n",
        "#\n",
        "# encoder101 = resnext101(replace_stride_with_dilation=[False, True, True])\n",
        "# #encoder50 = resnext50(sample_size=None, sample_duration=None, replace_stride_with_dilation=[False, True, True])\n",
        "# img = sitk.GetArrayFromImage(\n",
        "#     sitk.ReadImage('/Users/sha168/gitRepos/springfield_files2/data/CMR/cmr_MR_normalized/image_35.nii.gz'))\n",
        "# img = torch.from_numpy(img)\n",
        "# img = img[None, None].repeat([1, 3, 1, 1, 1])\n",
        "# fts101 = encoder101(img.float())\n",
        "# #fts50 = encoder50(img.float())\n",
        "# print('ResNeXt-101', img.shape, fts101.shape)\n",
        "# #print('ResNeXt-50', img.shape, fts50.shape)\n"
      ],
      "metadata": {
        "id": "sqJjQbMOdJWt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "# from .backbone.resnext3D import resnext101\n",
        "\n",
        "\n",
        "class FewShotSeg(nn.Module):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(resnext101(replace_stride_with_dilation=[False, True, True]),\n",
        "                                     nn.Conv3d(2048, 256, kernel_size=1, stride=1, bias=False))\n",
        "        self.device = torch.device('cuda')\n",
        "        self.t = Parameter(torch.Tensor([-10.0]))\n",
        "        self.scaler = 20.0\n",
        "        self.criterion = nn.NLLLoss()\n",
        "\n",
        "    def forward(self, supp_imgs, fore_mask, qry_imgs, train=False, t_loss_scaler=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            supp_imgs: support images\n",
        "                way x shot x [B x 3 x H x W], list of lists of tensors\n",
        "            fore_mask: foreground masks for support images\n",
        "                way x shot x [B x H x W], list of lists of tensors\n",
        "            back_mask: background masks for support images\n",
        "                way x shot x [B x H x W], list of lists of tensors\n",
        "            qry_imgs: query images\n",
        "                N x [B x 3 x H x W], list of tensors\n",
        "        \"\"\"\n",
        "\n",
        "        n_ways = len(supp_imgs)\n",
        "        self.n_shots = len(supp_imgs[0])\n",
        "        n_queries = len(qry_imgs)\n",
        "        batch_size = supp_imgs[0][0].shape[0]\n",
        "        img_size = qry_imgs[0].shape[-3:]\n",
        "\n",
        "        # ###### Extract features ######\n",
        "        s_imgs_concat = torch.cat([torch.stack(way, dim=0) for way in supp_imgs], dim=0)\n",
        "        q_imgs_concat = torch.cat(qry_imgs, dim=0)\n",
        "\n",
        "        s_img_fts = self.encoder(s_imgs_concat.repeat([1, 3, 1, 1, 1]))\n",
        "        q_img_fts = self.encoder(q_imgs_concat.repeat([1, 3, 1, 1, 1]))\n",
        "\n",
        "        s_fts_size = s_img_fts.shape[-3:]\n",
        "        q_fts_size = q_img_fts.shape[-3:]\n",
        "\n",
        "        supp_fts = s_img_fts.view(\n",
        "            n_ways, self.n_shots, batch_size, -1, *s_fts_size)  # Wa x Sh x B x C x D' x H' x W'\n",
        "        qry_fts = q_img_fts.view(\n",
        "            n_queries, batch_size, -1, *q_fts_size)  # N x B x C x D' x H' x W'\n",
        "\n",
        "        fore_mask = torch.stack([torch.stack(way, dim=0)\n",
        "                                 for way in fore_mask], dim=0)  # Wa x Sh x B x H' x W'\n",
        "\n",
        "        ###### Compute loss ######\n",
        "        align_loss = torch.zeros(1).to(self.device)\n",
        "        outputs = []\n",
        "        for epi in range(batch_size):\n",
        "\n",
        "            ###### Extract prototypes ######\n",
        "            supp_fts_ = [[self.getFeatures(supp_fts[way, shot, [epi]],\n",
        "                                           fore_mask[way, shot, [epi]])\n",
        "                          for shot in range(self.n_shots)] for way in range(n_ways)]\n",
        "\n",
        "            fg_prototypes = self.getPrototype(supp_fts_)\n",
        "\n",
        "            ###### Compute anom. scores ######\n",
        "            anom_s = [self.negSim(qry_fts[:, epi], prototype) for prototype in fg_prototypes]\n",
        "\n",
        "            ###### Get threshold #######\n",
        "            self.thresh_pred = [self.t for _ in range(n_ways)]\n",
        "            self.t_loss = self.t / self.scaler\n",
        "\n",
        "            ###### Get predictions #######\n",
        "            pred = self.getPred(anom_s, self.thresh_pred)  # N x Wa x H' x W'\n",
        "\n",
        "            pred_ups = F.interpolate(pred, size=img_size, mode='trilinear', align_corners=True)\n",
        "            pred_ups = torch.cat((1.0 - pred_ups, pred_ups), dim=1)\n",
        "\n",
        "            outputs.append(pred_ups)\n",
        "\n",
        "            ###### Prototype alignment loss ######\n",
        "            if train:\n",
        "                align_loss_epi = self.alignLoss(qry_fts[:, epi], torch.cat((1.0 - pred, pred), dim=1),\n",
        "                                                supp_fts[:, :, epi],\n",
        "                                                fore_mask[:, :, epi])\n",
        "                align_loss += align_loss_epi\n",
        "\n",
        "        output = torch.stack(outputs, dim=1)  # N x B x (1 + Wa) x H x W\n",
        "        output = output.view(-1, *output.shape[2:])\n",
        "        return output, (align_loss / batch_size), (t_loss_scaler * self.t_loss)\n",
        "\n",
        "    def negSim(self, fts, prototype):\n",
        "        \"\"\"\n",
        "        Calculate the distance between features and prototypes\n",
        "\n",
        "        Args:\n",
        "            fts: input features\n",
        "                expect shape: N x C x H x W\n",
        "            prototype: prototype of one semantic class\n",
        "                expect shape: 1 x C\n",
        "        \"\"\"\n",
        "\n",
        "        sim = - F.cosine_similarity(fts, prototype[..., None, None, None], dim=1) * self.scaler\n",
        "\n",
        "        return sim\n",
        "\n",
        "    def getFeatures(self, fts, mask):\n",
        "        \"\"\"\n",
        "        Extract foreground and background features via masked average pooling\n",
        "\n",
        "        Args:\n",
        "            fts: input features, expect shape: 1 x C x H' x W'\n",
        "            mask: binary mask, expect shape: 1 x H x W\n",
        "        \"\"\"\n",
        "\n",
        "        #fts = F.interpolate(fts, size=mask.shape[-3:], mode='trilinear')\n",
        "        mask = F.interpolate(mask[None], size=fts.shape[-3:], mode='nearest')[0]\n",
        "\n",
        "        # masked fg features\n",
        "        masked_fts = torch.sum(fts * mask[None, ...], dim=(2, 3, 4)) \\\n",
        "                     / (mask[None, ...].sum(dim=(2, 3, 4)) + 1e-5)  # 1 x C\n",
        "\n",
        "        return masked_fts\n",
        "\n",
        "    def getPrototype(self, fg_fts):\n",
        "        \"\"\"\n",
        "        Average the features to obtain the prototype\n",
        "\n",
        "        Args:\n",
        "            fg_fts: lists of list of foreground features for each way/shot\n",
        "                expect shape: Wa x Sh x [1 x C]\n",
        "            bg_fts: lists of list of background features for each way/shot\n",
        "                expect shape: Wa x Sh x [1 x C]\n",
        "        \"\"\"\n",
        "\n",
        "        n_ways, n_shots = len(fg_fts), len(fg_fts[0])\n",
        "        fg_prototypes = [torch.sum(torch.cat([tr for tr in way], dim=0), dim=0, keepdim=True) / n_shots for way in\n",
        "                         fg_fts]  ## concat all fg_fts\n",
        "\n",
        "        return fg_prototypes\n",
        "\n",
        "    def alignLoss(self, qry_fts, pred, supp_fts, fore_mask):\n",
        "\n",
        "        n_ways, n_shots = len(fore_mask), len(fore_mask[0])\n",
        "        fore_mask = F.interpolate(fore_mask, size=qry_fts.shape[-3:], mode='nearest')\n",
        "\n",
        "        # Mask and get query prototype\n",
        "        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n",
        "        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n",
        "        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n",
        "        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n",
        "\n",
        "        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3, 4, 5))\n",
        "        qry_prototypes = qry_prototypes / (pred_mask.sum((0, 3, 4, 5)) + 1e-5)  # (1 + Wa) x C\n",
        "\n",
        "        # Compute the support loss\n",
        "        loss = torch.zeros(1).to(self.device)\n",
        "        for way in range(n_ways):\n",
        "            if way in skip_ways:\n",
        "                continue\n",
        "            # Get the query prototypes\n",
        "            for shot in range(n_shots):\n",
        "                img_fts = supp_fts[way, [shot]]\n",
        "                supp_sim = self.negSim(img_fts, qry_prototypes[[way + 1]])\n",
        "\n",
        "                pred = self.getPred([supp_sim], [self.thresh_pred[way]])  # N x Wa x H' x W'\n",
        "                pred_ups = torch.cat((1.0 - pred, pred), dim=1)\n",
        "\n",
        "                # Construct the support Ground-Truth segmentation\n",
        "                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n",
        "                supp_label[fore_mask[way, shot] == 1] = 1\n",
        "                supp_label[fore_mask[way, shot] == 0] = 0\n",
        "\n",
        "                # Compute Loss\n",
        "                eps = torch.finfo(torch.float32).eps\n",
        "                log_prob = torch.log(torch.clamp(pred_ups, eps, 1 - eps))\n",
        "                loss += self.criterion(log_prob, supp_label[None, ...].long()) / n_shots / n_ways\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def getPred(self, sim, thresh):\n",
        "        pred = []\n",
        "\n",
        "        for s, t in zip(sim, thresh):\n",
        "            pred.append(1.0 - torch.sigmoid(0.5 * (s - t)))\n",
        "\n",
        "        return torch.stack(pred, dim=1)  # N x Wa x H' x W'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AQQXVM-NdTd6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import tqdm\n",
        "\n",
        "# from models.fewshot_anom_3D import FewShotSeg\n",
        "# from dataloading.datasets_3D import TrainDataset, TestDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from utils import *\n",
        "from dataloading.dataset_specifics import *\n",
        "\n",
        "def main():\n",
        "    # args = parse_arguments()\n",
        "\n",
        "    # Deterministic setting for reproducability.\n",
        "    # if args.seed is not None:\n",
        "    #     random.seed(args.seed)\n",
        "    #     torch.manual_seed(args.seed)\n",
        "    #     cudnn.deterministic = True\n",
        "\n",
        "    # Set up logging.\n",
        "    logger = set_logger(args.save_root, 'train.log')\n",
        "    logger.info(args)\n",
        "\n",
        "    # Setup the path to save.\n",
        "    args.save_model_path = os.path.join(args.save_root, 'model.pth')\n",
        "\n",
        "    # Init model.\n",
        "    model = FewShotSeg(args)\n",
        "    model = nn.DataParallel(model.cuda())\n",
        "\n",
        "    # Init optimizer.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    milestones = [(ii + 1) * 1000 for ii in range(args.steps // 1000 - 1)]\n",
        "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=args.lr_gamma)\n",
        "\n",
        "    # Define loss function.\n",
        "    my_weight = torch.FloatTensor([args.bg_wt, 1.0]).cuda()\n",
        "    criterion = nn.NLLLoss(ignore_index=255, weight=my_weight)\n",
        "\n",
        "    # Enable cuDNN benchmark mode to select the fastest convolution algorithm.\n",
        "    cudnn.enabled = True\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Define data set and loader.\n",
        "    train_dataset = TrainDataset(args)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                                               num_workers=args.workers, pin_memory=True, drop_last=True)\n",
        "    logger.info('  Training on images not in test fold: ' +\n",
        "                str([elem[len(args.data_root):] for elem in train_dataset.image_dirs]))\n",
        "\n",
        "    # Start training.\n",
        "    sub_epochs = args.steps // args.max_iterations\n",
        "    logger.info('  Start training ...')\n",
        "\n",
        "    for epoch in range(sub_epochs):\n",
        "        # Train.\n",
        "        batch_time, data_time, losses, q_loss, align_loss, t_loss = train(train_loader, model, criterion, optimizer,\n",
        "                                                                          scheduler, epoch, args)\n",
        "\n",
        "        # Log\n",
        "        logger.info('============== Epoch [{}] =============='.format(epoch))\n",
        "        logger.info('  Batch time: {:6.3f}'.format(batch_time))\n",
        "        logger.info('  Loading time: {:6.3f}'.format(data_time))\n",
        "        logger.info('  Total Loss  : {:.5f}'.format(losses))\n",
        "        logger.info('  Query Loss  : {:.5f}'.format(q_loss))\n",
        "        logger.info('  Align Loss  : {:.5f}'.format(align_loss))\n",
        "        logger.info('  Threshold Loss  : {:.5f}'.format(t_loss))\n",
        "\n",
        "        if epoch == 29:\n",
        "            torch.save(model.state_dict(), args.save_model_path)\n",
        "\n",
        "    # Save trained model.\n",
        "    logger.info('  Saving model ...')\n",
        "    torch.save(model.state_dict(), args.save_model_path)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    q_loss = AverageMeter('Query loss', ':.4f')\n",
        "    a_loss = AverageMeter('Align loss', ':.4f')\n",
        "    t_loss = AverageMeter('Threshold loss', ':.4f')\n",
        "\n",
        "    # Train mode.\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    # for i, sample in enumerate(train_loader):\n",
        "    for i, sample in enumerate(tqdm.tqdm(train_loader)):\n",
        "\n",
        "        # Extract episode data.\n",
        "        support_images = [[shot[None].float().cuda() for shot in way]\n",
        "                          for way in sample['support_images']]\n",
        "        support_fg_mask = [[shot[None].float().cuda() for shot in way]\n",
        "                           for way in sample['support_fg_labels']]\n",
        "\n",
        "        query_images = [query_image.float().cuda() for query_image in sample['query_images']]\n",
        "        query_labels = torch.cat([query_label.long().cuda() for query_label in sample['query_labels']], dim=0)\n",
        "\n",
        "        # Log loading time.\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # Compute outputs and losses.\n",
        "        query_pred, align_loss, thresh_loss = model(support_images, support_fg_mask, query_images,\n",
        "                                                    train=True, t_loss_scaler=args.t_loss_scaler)\n",
        "\n",
        "        query_loss = criterion(torch.log(torch.clamp(query_pred, torch.finfo(torch.float32).eps,\n",
        "                                                     1 - torch.finfo(torch.float32).eps)), query_labels[None])\n",
        "        loss = query_loss + align_loss + thresh_loss\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        for param in model.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log loss.\n",
        "        losses.update(loss.item(), query_pred.size(0))\n",
        "        q_loss.update(query_loss.item(), query_pred.size(0))\n",
        "        a_loss.update(align_loss.item(), query_pred.size(0))\n",
        "        t_loss.update(thresh_loss.item(), query_pred.size(0))\n",
        "\n",
        "        # Log elapsed time.\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "    return batch_time.avg, data_time.avg, losses.avg, q_loss.avg, a_loss.avg, t_loss.avg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "ZC2TUch3XfUM",
        "outputId": "a624e64f-7236-4561-b05a-f1463ce088e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/ADNet-VIN/log_out'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-755bff35524a>\u001b[0m in \u001b[0;36m<cell line: 145>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-755bff35524a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Set up logging.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ADNet-VIN/utils.py\u001b[0m in \u001b[0;36mset_logger\u001b[0;34m(log_path, file_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/ADNet-VIN/log_out'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the folder path to remove\n",
        "folder_path = '/content/ADNet-VIN/log_out'\n",
        "\n",
        "# Remove the folder using a shell command\n",
        "os.system(f'rm -r {folder_path}')\n",
        "\n",
        "print(f\"Folder '{folder_path}' has been removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U5CqCD57leU",
        "outputId": "ea37e085-fe7b-4a54-a6d6-ca6985e5b048"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/ADNet-VIN/log_out' has been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_names(dataset):\n",
        "    label_names = {}\n",
        "    if dataset == 'CMR':\n",
        "        label_names[0] = 'BG'\n",
        "        label_names[1] = 'LV-MYO'\n",
        "        label_names[2] = 'LV-BP'\n",
        "        label_names[3] = 'RV'\n",
        "\n",
        "    elif dataset == 'CHAOST2':\n",
        "        label_names[0] = 'BG'\n",
        "        label_names[1] = 'LIVER'\n",
        "        label_names[2] = 'RK'\n",
        "        label_names[3] = 'LK'\n",
        "        label_names[4] = 'SPLEEN'\n",
        "\n",
        "    return label_names\n"
      ],
      "metadata": {
        "id": "hADOFqviSg5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import tqdm\n",
        "\n",
        "# from models.fewshot_anom_3D import FewShotSeg\n",
        "# from dataloading.datasets_3D import TrainDataset, TestDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from utils import *\n",
        "from dataloading.dataset_specifics import *\n",
        "\n",
        "def main():\n",
        "    # args = parse_arguments()\n",
        "\n",
        "    # Deterministic setting for reproducability.\n",
        "    # if args.seed is not None:\n",
        "    #     random.seed(args.seed)\n",
        "    #     torch.manual_seed(args.seed)\n",
        "    #     cudnn.deterministic = True\n",
        "\n",
        "    # Set up logging.\n",
        "    logger = set_logger(args.save_root, 'train.log')\n",
        "    logger.info(args)\n",
        "\n",
        "    # Setup the path to save.\n",
        "    args.save_model_path = os.path.join(args.save_root, 'model.pth')\n",
        "\n",
        "    # Init model.\n",
        "    model = FewShotSeg(args)\n",
        "    model = torch.nn.DataParallel(model.cuda())\n",
        "    # model = model.cuda()\n",
        "\n",
        "    # Init optimizer.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    milestones = [(ii + 1) * 1000 for ii in range(args.steps // 1000 - 1)]\n",
        "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=args.lr_gamma)\n",
        "\n",
        "    # Define loss function.\n",
        "    my_weight = torch.FloatTensor([args.bg_wt, 1.0]).cuda()\n",
        "    criterion = nn.NLLLoss(ignore_index=255, weight=my_weight)\n",
        "\n",
        "    # Enable cuDNN benchmark mode to select the fastest convolution algorithm.\n",
        "    cudnn.enabled = True\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Define data set and loader.\n",
        "    train_dataset = TrainDataset(args)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                                               num_workers=args.workers, pin_memory=True, drop_last=True)\n",
        "\n",
        "    test_dataset = TestDataset(args)\n",
        "    query_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=False,\n",
        "                              num_workers=args.workers,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    labels = get_label_names(args.dataset)\n",
        "\n",
        "\n",
        "    # Start training.\n",
        "    sub_epochs = args.steps // args.max_iterations\n",
        "    logger.info('  Start training ...')\n",
        "\n",
        "    for epoch in range(10):\n",
        "        # Train.\n",
        "        batch_time, data_time, losses, q_loss, align_loss, t_loss = train(train_loader, model, criterion, optimizer,\n",
        "                                                                          scheduler, epoch, args)\n",
        "\n",
        "        # Log\n",
        "        logger.info('============== Epoch [{}] =============='.format(epoch))\n",
        "        logger.info('Batch time: {:6.3f}'.format(batch_time))\n",
        "        logger.info('Loading time: {:6.3f}'.format(data_time))\n",
        "        logger.info('Total Loss  : {:.5f}'.format(losses))\n",
        "        logger.info('Query Loss  : {:.5f}'.format(q_loss))\n",
        "        logger.info('Align Loss  : {:.5f}'.format(align_loss))\n",
        "        logger.info('Threshold Loss  : {:.5f}'.format(t_loss))\n",
        "\n",
        "        if epoch %1 == 0:\n",
        "            torch.save(model.state_dict(), args.save_model_path)\n",
        "\n",
        "#         # Infer.\n",
        "#         # Get support sample + mask for current class.\n",
        "\n",
        "#        # Loop over classes.\n",
        "#         class_dice = {}\n",
        "#         class_iou = {}\n",
        "#         for label_val, label_name in labels.items():\n",
        "#     # Skip BG class.\n",
        "#           if label_name is 'BG':\n",
        "#             continue\n",
        "#           logger.info('*------------------Class: {}--------------------*'.format(label_name))\n",
        "#           logger.info('*--------------------------------------------------*')\n",
        "\n",
        "#     # Get support sample + mask for current class.\n",
        "#           support_sample = test_dataset.getSupport(label=label_val, all_slices=args.all_slices, N=args.n_shot)\n",
        "#           test_dataset.label = label_val\n",
        "#     # Infer.\n",
        "#           with torch.no_grad():\n",
        "#               scores = infer(model, query_loader, support_sample, args, logger, label_name)\n",
        "\n",
        "#         # Log class-wise results\n",
        "#           class_dice[label_name] = torch.tensor(scores.patient_dice).mean().item()\n",
        "#           class_iou[label_name] = torch.tensor(scores.patient_iou).mean().item()\n",
        "\n",
        "#           logger.info('Mean class IoU: {}'.format(class_iou[label_name]))\n",
        "#           logger.info('Mean class Dice: {}'.format(class_dice[label_name]))\n",
        "#           logger.info('*--------------------------------------------------*')\n",
        "\n",
        "#     # Save trained model.\n",
        "#         logger.info('  Saving model ...')\n",
        "#         torch.save(model.state_dict(), args.save_model_path)\n",
        "\n",
        "# def infer(model, query_loader, support_sample, args, logger, label_name):\n",
        "\n",
        "#     # Test mode.\n",
        "#     model.eval()\n",
        "\n",
        "#     # Unpack support data.\n",
        "#     support_image = [support_sample['image'][[i]].float().cuda() for i in range(support_sample['image'].shape[0])]  # n_shot x 3 x H x W\n",
        "#     support_fg_mask = [support_sample['label'][[i]].float().cuda() for i in range(support_sample['image'].shape[0])]  # n_shot x H x W\n",
        "\n",
        "#     # Loop through query volumes.\n",
        "#     scores = Scores()\n",
        "#     for i, sample in enumerate(query_loader):\n",
        "\n",
        "#         # Unpack query data.\n",
        "#         query_image = [sample['image'][i].float().cuda() for i in range(sample['image'].shape[0])]  # [C x 3 x H x W]\n",
        "#         query_label = sample['label'].long()  # C x H x W\n",
        "#         query_id = sample['id'][0].split('image_')[1][:-len('.nii.gz')]\n",
        "\n",
        "#         # Compute output.\n",
        "#         if args.EP1 is True:\n",
        "#             # Match support slice and query sub-chunck.\n",
        "#             query_pred = torch.zeros(query_label.shape[-3:])\n",
        "#             C_q = sample['image'].shape[1]\n",
        "#             idx_ = np.linspace(0, C_q, args.n_shot+1).astype('int')\n",
        "#             for sub_chunck in range(args.n_shot):\n",
        "#                 support_image_s = [support_image[sub_chunck]]  # 1 x 3 x H x W\n",
        "#                 support_fg_mask_s = [support_fg_mask[sub_chunck]]  # 1 x H x W\n",
        "#                 query_image_s = query_image[0][idx_[sub_chunck]:idx_[sub_chunck+1]]  # C' x 3 x H x W\n",
        "#                 query_pred_s, _, _ = model([support_image_s], [support_fg_mask_s], [query_image_s], train=False)  # C x 2 x H x W\n",
        "#                 query_pred_s = query_pred_s.argmax(dim=1).cpu()  # C x H x W\n",
        "#                 query_pred[idx_[sub_chunck]:idx_[sub_chunck+1]] = query_pred_s\n",
        "\n",
        "#         else:  # EP 2\n",
        "#             query_pred, _, _ = model([support_image], [support_fg_mask], query_image, train=False)  # C x 2 x H x W\n",
        "#             query_pred = query_pred.argmax(dim=1).cpu()  # C x H x W\n",
        "\n",
        "#         # Record scores.\n",
        "#         scores.record(query_pred, query_label)\n",
        "\n",
        "#         # Log.\n",
        "#         logger.info('    Tested query volume: ' + sample['id'][0][len(args.data_root):]\n",
        "#                     + '. Dice score:  ' + str(scores.patient_dice[-1].item()))\n",
        "\n",
        "#         # Save predictions.\n",
        "#         file_name = 'image_' + query_id + '_' + label_name + '.pt'\n",
        "#         torch.save(query_pred, os.path.join(args.save, file_name))\n",
        "\n",
        "#     return scores\n",
        "\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    q_loss = AverageMeter('Query loss', ':.4f')\n",
        "    a_loss = AverageMeter('Align loss', ':.4f')\n",
        "    t_loss = AverageMeter('Threshold loss', ':.4f')\n",
        "\n",
        "    # Train mode.\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    # for i, sample in enumerate(train_loader):\n",
        "    for i, sample in enumerate(tqdm.tqdm(train_loader)):\n",
        "\n",
        "        # Extract episode data.\n",
        "        support_images = [[shot[None].float().cuda() for shot in way]\n",
        "                          for way in sample['support_images']]\n",
        "        support_fg_mask = [[shot[None].float().cuda() for shot in way]\n",
        "                           for way in sample['support_fg_labels']]\n",
        "\n",
        "        query_images = [query_image.float().cuda() for query_image in sample['query_images']]\n",
        "        query_labels = torch.cat([query_label.long().cuda() for query_label in sample['query_labels']], dim=0)\n",
        "\n",
        "        # Log loading time.\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        # Compute outputs and losses.\n",
        "        query_pred, align_loss, thresh_loss = model(support_images, support_fg_mask, query_images,\n",
        "                                                    train=True, t_loss_scaler=args.t_loss_scaler)\n",
        "\n",
        "        query_loss = criterion(torch.log(torch.clamp(query_pred, torch.finfo(torch.float32).eps,\n",
        "                                                     1 - torch.finfo(torch.float32).eps)), query_labels[None])\n",
        "        loss = query_loss + align_loss + thresh_loss\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        for param in model.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log loss.\n",
        "        losses.update(loss.item(), query_pred.size(0))\n",
        "        q_loss.update(query_loss.item(), query_pred.size(0))\n",
        "        a_loss.update(align_loss.item(), query_pred.size(0))\n",
        "        t_loss.update(thresh_loss.item(), query_pred.size(0))\n",
        "\n",
        "        # Log elapsed time.\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "    return batch_time.avg, data_time.avg, losses.avg, q_loss.avg, a_loss.avg, t_loss.avg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsfc3V477fof",
        "outputId": "f6ae054b-ee4d-4848-8a5e-dd4e309df874"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Namespace(data_root='/content/ADNet-VIN/dataloading/', save_root='/content/ADNet-VIN/log_out', dataset='CHAOST2', n_sv=1, fold=1, max_slices=10, workers=4, steps=15000, n_shot=1, n_query=1, n_way=1, batch_size=1, max_iterations=50, lr=0.001, lr_gamma=0.95, momentum=0.9, weight_decay=0.0005, seed=None, bg_wt=0.1, t_loss_scaler=1.0, min_size=200, all_slices=True, EP1=True)\n",
            "Namespace(data_root='/content/ADNet-VIN/dataloading/', save_root='/content/ADNet-VIN/log_out', dataset='CHAOST2', n_sv=1, fold=1, max_slices=10, workers=4, steps=15000, n_shot=1, n_query=1, n_way=1, batch_size=1, max_iterations=50, lr=0.001, lr_gamma=0.95, momentum=0.9, weight_decay=0.0005, seed=None, bg_wt=0.1, t_loss_scaler=1.0, min_size=200, all_slices=True, EP1=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained weights!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-3a6eb28930f5>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_dict = torch.load('/content/resnext-101-kinetics.pth', map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "INFO:root:  Start training ...\n",
            "  Start training ...\n",
            "100%|| 50/50 [03:54<00:00,  4.69s/it]\n",
            "INFO:root:============== Epoch [0] ==============\n",
            "============== Epoch [0] ==============\n",
            "INFO:root:Batch time:  4.685\n",
            "Batch time:  4.685\n",
            "INFO:root:Loading time:  2.217\n",
            "Loading time:  2.217\n",
            "INFO:root:Total Loss  : 0.91030\n",
            "Total Loss  : 0.91030\n",
            "INFO:root:Query Loss  : 0.74235\n",
            "Query Loss  : 0.74235\n",
            "INFO:root:Align Loss  : 0.67075\n",
            "Align Loss  : 0.67075\n",
            "INFO:root:Threshold Loss  : -0.50280\n",
            "Threshold Loss  : -0.50280\n",
            "100%|| 50/50 [05:14<00:00,  6.28s/it]\n",
            "INFO:root:============== Epoch [1] ==============\n",
            "============== Epoch [1] ==============\n",
            "INFO:root:Batch time:  6.279\n",
            "Batch time:  6.279\n",
            "INFO:root:Loading time:  4.252\n",
            "Loading time:  4.252\n",
            "INFO:root:Total Loss  : 0.46326\n",
            "Total Loss  : 0.46326\n",
            "INFO:root:Query Loss  : 0.62491\n",
            "Query Loss  : 0.62491\n",
            "INFO:root:Align Loss  : 0.34546\n",
            "Align Loss  : 0.34546\n",
            "INFO:root:Threshold Loss  : -0.50712\n",
            "Threshold Loss  : -0.50712\n",
            "100%|| 50/50 [03:37<00:00,  4.35s/it]\n",
            "INFO:root:============== Epoch [2] ==============\n",
            "============== Epoch [2] ==============\n",
            "INFO:root:Batch time:  4.348\n",
            "Batch time:  4.348\n",
            "INFO:root:Loading time:  2.343\n",
            "Loading time:  2.343\n",
            "INFO:root:Total Loss  : 0.41465\n",
            "Total Loss  : 0.41465\n",
            "INFO:root:Query Loss  : 0.54020\n",
            "Query Loss  : 0.54020\n",
            "INFO:root:Align Loss  : 0.38404\n",
            "Align Loss  : 0.38404\n",
            "INFO:root:Threshold Loss  : -0.50958\n",
            "Threshold Loss  : -0.50958\n",
            "100%|| 50/50 [04:26<00:00,  5.34s/it]\n",
            "INFO:root:============== Epoch [3] ==============\n",
            "============== Epoch [3] ==============\n",
            "INFO:root:Batch time:  5.332\n",
            "Batch time:  5.332\n",
            "INFO:root:Loading time:  3.322\n",
            "Loading time:  3.322\n",
            "INFO:root:Total Loss  : 0.47732\n",
            "Total Loss  : 0.47732\n",
            "INFO:root:Query Loss  : 0.64250\n",
            "Query Loss  : 0.64250\n",
            "INFO:root:Align Loss  : 0.34775\n",
            "Align Loss  : 0.34775\n",
            "INFO:root:Threshold Loss  : -0.51293\n",
            "Threshold Loss  : -0.51293\n",
            "100%|| 50/50 [04:31<00:00,  5.44s/it]\n",
            "INFO:root:============== Epoch [4] ==============\n",
            "============== Epoch [4] ==============\n",
            "INFO:root:Batch time:  5.430\n",
            "Batch time:  5.430\n",
            "INFO:root:Loading time:  3.420\n",
            "Loading time:  3.420\n",
            "INFO:root:Total Loss  : 0.32131\n",
            "Total Loss  : 0.32131\n",
            "INFO:root:Query Loss  : 0.53784\n",
            "Query Loss  : 0.53784\n",
            "INFO:root:Align Loss  : 0.29902\n",
            "Align Loss  : 0.29902\n",
            "INFO:root:Threshold Loss  : -0.51555\n",
            "Threshold Loss  : -0.51555\n",
            "100%|| 50/50 [03:37<00:00,  4.35s/it]\n",
            "INFO:root:============== Epoch [5] ==============\n",
            "============== Epoch [5] ==============\n",
            "INFO:root:Batch time:  4.343\n",
            "Batch time:  4.343\n",
            "INFO:root:Loading time:  2.350\n",
            "Loading time:  2.350\n",
            "INFO:root:Total Loss  : 0.29164\n",
            "Total Loss  : 0.29164\n",
            "INFO:root:Query Loss  : 0.51094\n",
            "Query Loss  : 0.51094\n",
            "INFO:root:Align Loss  : 0.29874\n",
            "Align Loss  : 0.29874\n",
            "INFO:root:Threshold Loss  : -0.51804\n",
            "Threshold Loss  : -0.51804\n",
            "100%|| 50/50 [03:52<00:00,  4.65s/it]\n",
            "INFO:root:============== Epoch [6] ==============\n",
            "============== Epoch [6] ==============\n",
            "INFO:root:Batch time:  4.649\n",
            "Batch time:  4.649\n",
            "INFO:root:Loading time:  2.637\n",
            "Loading time:  2.637\n",
            "INFO:root:Total Loss  : 0.25606\n",
            "Total Loss  : 0.25606\n",
            "INFO:root:Query Loss  : 0.50164\n",
            "Query Loss  : 0.50164\n",
            "INFO:root:Align Loss  : 0.27502\n",
            "Align Loss  : 0.27502\n",
            "INFO:root:Threshold Loss  : -0.52060\n",
            "Threshold Loss  : -0.52060\n",
            "100%|| 50/50 [04:19<00:00,  5.18s/it]\n",
            "INFO:root:============== Epoch [7] ==============\n",
            "============== Epoch [7] ==============\n",
            "INFO:root:Batch time:  5.180\n",
            "Batch time:  5.180\n",
            "INFO:root:Loading time:  3.152\n",
            "Loading time:  3.152\n",
            "INFO:root:Total Loss  : 0.28244\n",
            "Total Loss  : 0.28244\n",
            "INFO:root:Query Loss  : 0.52397\n",
            "Query Loss  : 0.52397\n",
            "INFO:root:Align Loss  : 0.28134\n",
            "Align Loss  : 0.28134\n",
            "INFO:root:Threshold Loss  : -0.52287\n",
            "Threshold Loss  : -0.52287\n",
            "100%|| 50/50 [03:38<00:00,  4.37s/it]\n",
            "INFO:root:============== Epoch [8] ==============\n",
            "============== Epoch [8] ==============\n",
            "INFO:root:Batch time:  4.363\n",
            "Batch time:  4.363\n",
            "INFO:root:Loading time:  2.355\n",
            "Loading time:  2.355\n",
            "INFO:root:Total Loss  : 0.11818\n",
            "Total Loss  : 0.11818\n",
            "INFO:root:Query Loss  : 0.41549\n",
            "Query Loss  : 0.41549\n",
            "INFO:root:Align Loss  : 0.22771\n",
            "Align Loss  : 0.22771\n",
            "INFO:root:Threshold Loss  : -0.52502\n",
            "Threshold Loss  : -0.52502\n",
            "100%|| 50/50 [03:40<00:00,  4.41s/it]\n",
            "INFO:root:============== Epoch [9] ==============\n",
            "============== Epoch [9] ==============\n",
            "INFO:root:Batch time:  4.402\n",
            "Batch time:  4.402\n",
            "INFO:root:Loading time:  2.398\n",
            "Loading time:  2.398\n",
            "INFO:root:Total Loss  : 0.15301\n",
            "Total Loss  : 0.15301\n",
            "INFO:root:Query Loss  : 0.42291\n",
            "Query Loss  : 0.42291\n",
            "INFO:root:Align Loss  : 0.25723\n",
            "Align Loss  : 0.25723\n",
            "INFO:root:Threshold Loss  : -0.52713\n",
            "Threshold Loss  : -0.52713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from models.fewshot_anom_3D import FewShotSeg\n",
        "# from dataloading.datasets_3D import TestDataset\n",
        "from dataloading.dataset_specifics import *\n",
        "from utils import *\n",
        "\n",
        "\n",
        "# def parse_arguments():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--data_root', type=str, required=True)\n",
        "#     parser.add_argument('--save_root', type=str, required=True)\n",
        "#     parser.add_argument('--pretrained_root', type=str, required=True)\n",
        "#     parser.add_argument('--fold', type=int, required=True)\n",
        "#     parser.add_argument('--dataset', type=str, required=True)\n",
        "#     parser.add_argument('--n_shot', default=1, type=int)\n",
        "#     parser.add_argument('--all_slices', default=True, type=bool)\n",
        "#     parser.add_argument('--EP1', default=False, type=bool)\n",
        "#     parser.add_argument('--seed', default=None, type=int)\n",
        "#     parser.add_argument('--workers', default=0, type=int)\n",
        "\n",
        "#     return parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args, unknown = parse_arguments()\n",
        "    # Deterministic setting for reproducability.\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        cudnn.deterministic = True\n",
        "\n",
        "    # Setup the path to save.\n",
        "    args.save = args.save_root\n",
        "\n",
        "    logger = set_logger('/content/ADNet-VIN/infer_out/', 'infer.log')\n",
        "    logger.info(args)\n",
        "\n",
        "    # Init model and load state_dict.\n",
        "    model = FewShotSeg(args)\n",
        "    model = nn.DataParallel(model.cuda())\n",
        "    # model.load_state_dict(torch.load(args.pretrained_root, map_location=\"cpu\"))\n",
        "    model.load_state_dict(torch.load('/content/ADNet-VIN/log_out/model.pth', map_location=\"cpu\"))\n",
        "\n",
        "    # Data loader.\n",
        "    test_dataset = TestDataset(args)\n",
        "    query_loader = DataLoader(test_dataset,\n",
        "                              batch_size=1,\n",
        "                              shuffle=False,\n",
        "                              num_workers=args.workers,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    # Inference.\n",
        "    logger.info('  Start inference ... Note: EP1 is ' + str(args.EP1))\n",
        "    logger.info('  Support: ' + str(test_dataset.support_dir[len(args.data_root):]))\n",
        "    logger.info('  Query: ' +\n",
        "                str([elem[len(args.data_root):] for elem in test_dataset.image_dirs]))\n",
        "\n",
        "    # Get unique labels (classes).\n",
        "    labels = get_label_names(args.dataset)\n",
        "\n",
        "    # Loop over classes.\n",
        "    class_dice = {}\n",
        "    class_iou = {}\n",
        "    for label_val, label_name in labels.items():\n",
        "\n",
        "        # Skip BG class.\n",
        "        if label_name is 'BG':\n",
        "            continue\n",
        "\n",
        "        logger.info('  *------------------Class: {}--------------------*'.format(label_name))\n",
        "        logger.info('  *--------------------------------------------------*')\n",
        "\n",
        "        # Get support sample + mask for current class.\n",
        "        support_sample = test_dataset.getSupport(label=label_val, all_slices=args.all_slices, N=args.n_shot)\n",
        "        test_dataset.label = label_val\n",
        "\n",
        "        # Infer.\n",
        "        with torch.no_grad():\n",
        "            scores = infer(model, query_loader, support_sample, args, logger, label_name)\n",
        "\n",
        "        # Log class-wise results\n",
        "        class_dice[label_name] = torch.tensor(scores.patient_dice).mean().item()\n",
        "        class_iou[label_name] = torch.tensor(scores.patient_iou).mean().item()\n",
        "\n",
        "        logger.info('Mean class IoU: {}'.format(class_iou[label_name]))\n",
        "        logger.info('Mean class Dice: {}'.format(class_dice[label_name]))\n",
        "        logger.info('  *--------------------------------------------------*')\n",
        "\n",
        "    # Log final results.\n",
        "    logger.info('  *-----------------Final results--------------------*')\n",
        "    logger.info('  *--------------------------------------------------*')\n",
        "    logger.info('  Mean IoU: {}'.format(class_iou))\n",
        "    logger.info('  Mean Dice: {}'.format(class_dice))\n",
        "    logger.info('  *--------------------------------------------------*')\n",
        "\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "def dice_score(pred, target, num_classes):\n",
        "    dice = []\n",
        "    for i in range(num_classes):\n",
        "        pred_i = (pred == i).float()\n",
        "        target_i = (target == i).float()\n",
        "        intersection = (pred_i * target_i).sum()\n",
        "        union = pred_i.sum() + target_i.sum()\n",
        "        dice.append((2. * intersection / union).item())\n",
        "    return dice\n",
        "\n",
        "def iou_score(pred, target, num_classes):\n",
        "    iou = []\n",
        "    for i in range(num_classes):\n",
        "        pred_i = (pred == i).float()\n",
        "        target_i = (target == i).float()\n",
        "        intersection = (pred_i * target_i).sum()\n",
        "        union = pred_i.sum() + target_i.sum() - intersection\n",
        "        iou.append((intersection / union).item())\n",
        "    return iou\n",
        "\n",
        "def infer(model, query_loader, support_sample, args, logger, label_name, num_classes):\n",
        "\n",
        "    # Test mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Unpack support data.\n",
        "    support_image = [support_sample['image'][i].float().cuda() for i in range(support_sample['image'].shape[0])]  # n_shot x 3 x H x W\n",
        "    support_fg_mask = [support_sample['label'][[i]].float().cuda() for i in range(support_sample['image'].shape[0])]  # n_shot x H x W\n",
        "\n",
        "    # Loop through query volumes.\n",
        "    scores = Scores()\n",
        "    for i, sample in enumerate(query_loader):\n",
        "\n",
        "        # Unpack query data.\n",
        "        query_image = [sample['image'][i].float().cuda() for i in range(sample['image'].shape[0])]  # [C x 3 x H x W]\n",
        "        query_label = sample['label'].long().cuda()  # C x H x W\n",
        "\n",
        "        query_id = sample['id'][0].split('image_')[1][:-len('.nii.gz')]\n",
        "\n",
        "        # Compute output.\n",
        "        if args.EP1:\n",
        "            # Match support slice and query sub-chunck.\n",
        "            query_pred = torch.zeros(query_label.shape[-3:], dtype=torch.long)\n",
        "            C_q = sample['image'].shape[1]\n",
        "            idx_ = np.linspace(0, C_q, args.n_shot+1).astype('int')\n",
        "            for sub_chunck in range(args.n_shot):\n",
        "                support_image_s = [support_image[sub_chunck]]  # 1 x 3 x H x W\n",
        "                support_fg_mask_s = [support_fg_mask[sub_chunck]]  # 1 x H x W\n",
        "                query_image_s = query_image[0][idx_[sub_chunck]:idx_[sub_chunck+1]]  # C' x 3 x H x W\n",
        "                query_pred_s, _, _ = model([support_image_s], [support_fg_mask_s], [query_image_s], train=False)  # C x num_classes x H x W\n",
        "                query_pred_s = query_pred_s.argmax(dim=1).cpu()  # C x H x W\n",
        "                query_pred[idx_[sub_chunck]:idx_[sub_chunck+1]] = query_pred_s\n",
        "\n",
        "        else:  # EP 2\n",
        "            query_pred_, _, _ = model([support_image], [support_fg_mask], query_image, train=False)  # C x num_classes x H x W\n",
        "            query_pred = query_pred_.argmax(dim=1).cpu()  # C x H x W\n",
        "\n",
        "        # Calculate metrics.\n",
        "        dice = dice_score(query_pred, query_label, num_classes)\n",
        "        iou = iou_score(query_pred, query_label, num_classes)\n",
        "\n",
        "        # Record scores.\n",
        "        scores.record(query_pred, query_label, dice, iou)\n",
        "\n",
        "        # Log.\n",
        "        logger.info('    Tested query volume: ' + sample['id'][0][len(args.data_root):]\n",
        "                    + '. Dice score:  ' + str(dice) + '. IoU score: ' + str(iou))\n",
        "\n",
        "        # Save predictions.\n",
        "        file_name = 'image_' + query_id + '_' + label_name + '.pt'\n",
        "        torch.save(query_pred, os.path.join(args.save, file_name))\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-3DONYB6oR6f",
        "outputId": "065e5ecf-39e6-44c2-ef9f-1e081a2c90ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:80: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:80: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-32-2f28ecd93509>:80: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if label_name is 'BG':\n",
            "INFO:root:Namespace(data_root='/content/ADNet-VIN/dataloading/', save_root='/content/ADNet-VIN/log_out', dataset='CHAOST2', n_sv=1, fold=1, max_slices=10, workers=4, steps=15000, n_shot=1, n_query=1, n_way=1, batch_size=1, max_iterations=50, lr=0.001, lr_gamma=0.95, momentum=0.9, weight_decay=0.0005, seed=None, bg_wt=0.1, t_loss_scaler=1.0, min_size=200, all_slices=True, EP1=True, save='/content/ADNet-VIN/log_out')\n",
            "Namespace(data_root='/content/ADNet-VIN/dataloading/', save_root='/content/ADNet-VIN/log_out', dataset='CHAOST2', n_sv=1, fold=1, max_slices=10, workers=4, steps=15000, n_shot=1, n_query=1, n_way=1, batch_size=1, max_iterations=50, lr=0.001, lr_gamma=0.95, momentum=0.9, weight_decay=0.0005, seed=None, bg_wt=0.1, t_loss_scaler=1.0, min_size=200, all_slices=True, EP1=True, save='/content/ADNet-VIN/log_out')\n",
            "Namespace(data_root='/content/ADNet-VIN/dataloading/', save_root='/content/ADNet-VIN/log_out', dataset='CHAOST2', n_sv=1, fold=1, max_slices=10, workers=4, steps=15000, n_shot=1, n_query=1, n_way=1, batch_size=1, max_iterations=50, lr=0.001, lr_gamma=0.95, momentum=0.9, weight_decay=0.0005, seed=None, bg_wt=0.1, t_loss_scaler=1.0, min_size=200, all_slices=True, EP1=True, save='/content/ADNet-VIN/log_out')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained weights!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-3a6eb28930f5>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_dict = torch.load('/content/resnext-101-kinetics.pth', map_location='cpu')\n",
            "<ipython-input-32-2f28ecd93509>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/ADNet-VIN/log_out/model.pth', map_location=\"cpu\"))\n",
            "INFO:root:  Start inference ... Note: EP1 is True\n",
            "  Start inference ... Note: EP1 is True\n",
            "  Start inference ... Note: EP1 is True\n",
            "INFO:root:  Support: chaos_MR_T2_normalized/image_19.nii.gz\n",
            "  Support: chaos_MR_T2_normalized/image_19.nii.gz\n",
            "  Support: chaos_MR_T2_normalized/image_19.nii.gz\n",
            "INFO:root:  Query: ['chaos_MR_T2_normalized/image_8.nii.gz', 'chaos_MR_T2_normalized/image_10.nii.gz', 'chaos_MR_T2_normalized/image_13.nii.gz', 'chaos_MR_T2_normalized/image_15.nii.gz']\n",
            "  Query: ['chaos_MR_T2_normalized/image_8.nii.gz', 'chaos_MR_T2_normalized/image_10.nii.gz', 'chaos_MR_T2_normalized/image_13.nii.gz', 'chaos_MR_T2_normalized/image_15.nii.gz']\n",
            "  Query: ['chaos_MR_T2_normalized/image_8.nii.gz', 'chaos_MR_T2_normalized/image_10.nii.gz', 'chaos_MR_T2_normalized/image_13.nii.gz', 'chaos_MR_T2_normalized/image_15.nii.gz']\n",
            "INFO:root:  *------------------Class: LIVER--------------------*\n",
            "  *------------------Class: LIVER--------------------*\n",
            "  *------------------Class: LIVER--------------------*\n",
            "INFO:root:  *--------------------------------------------------*\n",
            "  *--------------------------------------------------*\n",
            "  *--------------------------------------------------*\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/ADNet-VIN/dataloading/datasets_3D.py\", line 58, in __getitem__\n    sample['image'] = torch.from_numpy(img[idx])\nIndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 34\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2f28ecd93509>\u001b[0m in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-2f28ecd93509>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Infer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Log class-wise results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2f28ecd93509>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(model, query_loader, support_sample, args, logger, label_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Loop through query volumes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Unpack query data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/content/ADNet-VIN/dataloading/datasets_3D.py\", line 58, in __getitem__\n    sample['image'] = torch.from_numpy(img[idx])\nIndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 34\n"
          ]
        }
      ]
    }
  ]
}